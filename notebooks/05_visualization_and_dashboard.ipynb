{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb020965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\ramse\\xg-prediction-model\n"
     ]
    }
   ],
   "source": [
    "# Imports and helpers\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, precision_recall_curve, roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def find_repo_root(start=Path.cwd(), markers=('setup.py','requirements.txt','README.md')):\n",
    "    cur = start.resolve()\n",
    "    for _ in range(10):\n",
    "        if any((cur / m).exists() for m in markers):\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "print('Repo root:', repo_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20e2b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using processed CSV: C:\\Users\\ramse\\xg-prediction-model\\data\\processed\\processed_shots_20251221T175417Z.csv\n",
      "Features inferred: ['distance', 'minute_num', 'body_head', 'body_foot', 'body_other', 'big_chance', 'half', 'shot_type', 'assist_type']\n",
      "Loaded logistic\n",
      "Loaded random_forest\n",
      "Loaded xgboost\n",
      "Loaded neural_network\n"
     ]
    }
   ],
   "source": [
    "# Load processed data and available models (defensive)\n",
    "proc_dir = repo_root / 'data' / 'processed'\n",
    "files = sorted(proc_dir.glob('processed_shots_*.csv'), key=lambda p: p.stat().st_mtime) if proc_dir.exists() else []\n",
    "if not files:\n",
    "    raise FileNotFoundError('No processed_shots_*.csv files in data/processed/')\n",
    "data_path = files[-1]\n",
    "print('Using processed CSV:', data_path)\n",
    "df = pd.read_csv(data_path)\n",
    "if 'outcome' not in df.columns:\n",
    "    raise ValueError('processed data missing outcome column')\n",
    "y = df['outcome'].astype(str).str.lower().eq('goal').astype(int)\n",
    "# infer feature columns like training script\n",
    "numeric_feats = [c for c in ['distance','minute_num'] if c in df.columns]\n",
    "binary_feats = [c for c in ['body_head','body_foot','body_other','big_chance','half'] if c in df.columns]\n",
    "cat_feats = [c for c in ['shot_type','assist_type'] if c in df.columns]\n",
    "feature_cols = numeric_feats + binary_feats + cat_feats\n",
    "X = df[feature_cols].copy() if feature_cols else pd.DataFrame()\n",
    "print('Features inferred:', feature_cols)\n",
    "# build preprocessor as used in training\n",
    "num_pipe = Pipeline([('imp', SimpleImputer(strategy='median')), ('sc', StandardScaler())]) if numeric_feats else None\n",
    "bin_pipe = Pipeline([('imp', SimpleImputer(strategy='most_frequent'))]) if binary_feats else None\n",
    "cat_pipe = Pipeline([('imp', SimpleImputer(strategy='constant', fill_value='missing')), ('ohe', OneHotEncoder(handle_unknown='ignore'))]) if cat_feats else None\n",
    "transformers = []\n",
    "if numeric_feats: transformers.append(('num', num_pipe, numeric_feats))\n",
    "if binary_feats: transformers.append(('bin', bin_pipe, binary_feats))\n",
    "if cat_feats: transformers.append(('cat', cat_pipe, cat_feats))\n",
    "preprocessor = ColumnTransformer(transformers, remainder='drop') if transformers else None\n",
    "# locate models\n",
    "model_files = {\n",
    "    'logistic': repo_root / 'results' / 'metrics' / 'model_logistic_calibrated.joblib',\n",
    "    'random_forest': repo_root / 'results' / 'metrics' / 'model_random_forest_calibrated.joblib',\n",
    "    'xgboost': repo_root / 'results' / 'metrics' / 'model_xgboost_calibrated.joblib',\n",
    "    'neural_network': repo_root / 'results' / 'metrics' / 'model_neural_network_calibrated.joblib',\n",
    "}\n",
    "models = {}\n",
    "for name, p in model_files.items():\n",
    "    if p.exists():\n",
    "        try:\n",
    "            models[name] = joblib.load(p)\n",
    "            print('Loaded', name)\n",
    "        except Exception as e:\n",
    "            print('Failed to load', name, e)\n",
    "    else:\n",
    "        print('Model file missing:', p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331aedb",
   "metadata": {},
   "source": [
    "## 1) Model comparison: ROC & Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108123ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test split (same splitting logic as other notebooks) and plot ROC + calibration\n",
    "X_train_full, X_hold, y_train_full, y_hold = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_hold, y_hold, test_size=0.5, stratify=y_hold, random_state=42)\n",
    "if not models:\n",
    "    print('No models loaded; skipping comparison plots')\n",
    "else:\n",
    "    fig, axs = plt.subplots(1,2, figsize=(14,6))\n",
    "    # ROC on left\n",
    "    ax = axs[0]\n",
    "    for name, mdl in models.items():\n",
    "        try:\n",
    "            proba = mdl.predict_proba(X_test)[:,1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "            ax.plot(fpr, tpr, label=f'{name} (AUC={roc_auc_score(y_test, proba):.3f})')\n",
    "        except Exception as e:\n",
    "            print('Skipping ROC for', name, e)\n",
    "    ax.plot([0,1],[0,1],'k--', alpha=0.3)\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_ylabel('TPR')\n",
    "    ax.set_title('ROC curves (test set)')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True)\n",
    "    # Calibration on right\n",
    "    ax2 = axs[1]\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    for name, mdl in models.items():\n",
    "        try:\n",
    "            proba = mdl.predict_proba(X_test)[:,1]\n",
    "            prob_true, prob_pred = calibration_curve(y_test, proba, n_bins=10)\n",
    "            ax2.plot(prob_pred, prob_true, marker='o', label=f'{name} (Brier={brier_score_loss(y_test, proba):.3f})')\n",
    "        except Exception as e:\n",
    "            print('Skipping calibration for', name, e)\n",
    "    ax2.plot([0,1],[0,1],'k--', alpha=0.3)\n",
    "    ax2.set_xlabel('Mean predicted probability')\n",
    "    ax2.set_ylabel('Fraction of positives')\n",
    "    ax2.set_title('Calibration (reliability diagram)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    outdir = repo_root / 'results' / 'metrics' / 'figures'\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    savep = outdir / 'model_comparison_roc_calibration.png'\n",
    "    fig.savefig(savep, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved', savep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496ee39",
   "metadata": {},
   "source": [
    "## 2) Feature importance (models that expose importances or coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8932f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names_from_preprocessor(preproc):\n",
    "    try:\n",
    "        return list(preproc.get_feature_names_out())\n",
    "    except Exception:\n",
    "        names = []\n",
    "        if hasattr(preproc, 'transformers_'):\n",
    "            for name, trans, cols in preproc.transformers_:\n",
    "                if name == 'remainder' and trans == 'drop':\n",
    "                    continue\n",
    "                if hasattr(trans, 'named_steps') and 'ohe' in trans.named_steps:\n",
    "                    ohe = trans.named_steps['ohe']\n",
    "                    try:\n",
    "                        ohe_names = list(ohe.get_feature_names_out(cols))\n",
    "                        names.extend(ohe_names)\n",
    "                    except Exception:\n",
    "                        names.extend(cols)\n",
    "                else:\n",
    "                    names.extend(cols)\n",
    "        return names\n",
    "\n",
    "def unwrap_model(m):\n",
    "    try:\n",
    "        if hasattr(m, 'named_steps'):\n",
    "            clf = m.named_steps.get('clf') or m.named_steps.get('classifier') or list(m.named_steps.values())[-1]\n",
    "            m = clf\n",
    "    except Exception:\n",
    "        pass\n",
    "    if hasattr(m, 'base_estimator'):\n",
    "        return m.base_estimator\n",
    "    if hasattr(m, 'estimator'):\n",
    "        return m.estimator\n",
    "    return m\n",
    "\n",
    "feat_names = get_feature_names_from_preprocessor(preprocessor) if preprocessor is not None else feature_cols\n",
    "if not feat_names:\n",
    "    feat_names = feature_cols\n",
    "print('Final feature names used for importance mapping:', feat_names)\n",
    "\n",
    "imp_records = []\n",
    "for name, mdl in models.items():\n",
    "    try:\n",
    "        base = unwrap_model(mdl)\n",
    "        if hasattr(base, 'coef_'):\n",
    "            coefs = np.ravel(base.coef_)\n",
    "            if len(coefs) == len(feat_names):\n",
    "                df_imp = pd.DataFrame({'feature': feat_names, 'importance': np.abs(coefs)})\n",
    "                df_imp = df_imp.sort_values('importance', ascending=False).head(20)\n",
    "                imp_records.append((name, df_imp))\n",
    "        elif hasattr(base, 'feature_importances_'):\n",
    "            fi = base.feature_importances_\n",
    "            if len(fi) == len(feat_names):\n",
    "                df_imp = pd.DataFrame({'feature': feat_names, 'importance': fi})\n",
    "                df_imp = df_imp.sort_values('importance', ascending=False).head(20)\n",
    "                imp_records.append((name, df_imp))\n",
    "        else:\n",
    "            print('No direct importances for', name, '- skipping (try permutation importance offline)')\n",
    "    except Exception as e:\n",
    "        print('Error extracting importances for', name, e)\n",
    "\n",
    "if not imp_records:\n",
    "    print('No feature importances available to plot')\n",
    "else:\n",
    "    for name, df_imp in imp_records:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.barplot(data=df_imp, x='importance', y='feature')\n",
    "        plt.title(f'Top features: {name}')\n",
    "        outdir = repo_root / 'results' / 'metrics' / 'figures'\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "        p = outdir / f'feature_importance_{name}.png'\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(p, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print('Saved', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bc30a",
   "metadata": {},
   "source": [
    "## 3) Player / Team xG analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for player/team columns and aggregate xG vs actual goals\n",
    "player_cols = [c for c in ['player','player_name','shooter','playerId'] if c in df.columns]\n",
    "team_cols = [c for c in ['team','team_name','teamId'] if c in df.columns]\n",
    "group_by_player = player_cols[0] if player_cols else None\n",
    "group_by_team = team_cols[0] if team_cols else None\n",
    "# compute predicted probabilities using a prefered model if available\n",
    "if models:\n",
    "    pref = next((m for m in ['logistic','random_forest','xgboost','neural_network'] if m in models), None)\n",
    "    mapper = models[pref] if pref else list(models.values())[0]\n",
    "    try:\n",
    "        proba_all = mapper.predict_proba(X)[:,1] if (feature_cols and set(feature_cols).issubset(df.columns)) else mapper.predict_proba(X)[:,1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            proba_all = mapper.predict_proba(df)[:,1]\n",
    "        except Exception:\n",
    "            proba_all = np.full(len(df), np.nan)\n",
    "else:\n",
    "    proba_all = np.full(len(df), np.nan)\n",
    "df['_pred_xg'] = proba_all\n",
    "df['_is_goal'] = y\n",
    "if group_by_player:\n",
    "    gp = df.groupby(group_by_player).agg(xg=('_pred_xg','sum'), goals=('_is_goal','sum'), attempts=('_is_goal','count'))\n",
    "    gp = gp.sort_values('xg', ascending=False)\n",
    "    display(gp.head(10))\n",
    "    outp = repo_root / 'results' / 'metrics' / 'top_xg_by_player.csv'\n",
    "    gp.to_csv(outp)\n",
    "    print('Saved', outp)\n",
    "else:\n",
    "    print('No player column found; skipping player xG analysis')\n",
    "if group_by_team:\n",
    "    gt = df.groupby(group_by_team).agg(xg=('_pred_xg','sum'), goals=('_is_goal','sum'), attempts=('_is_goal','count'))\n",
    "    gt = gt.sort_values('xg', ascending=False)\n",
    "    display(gt.head(10))\n",
    "    outt = repo_root / 'results' / 'metrics' / 'top_xg_by_team.csv'\n",
    "    gt.to_csv(outt)\n",
    "    print('Saved', outt)\n",
    "else:\n",
    "    print('No team column found; skipping team xG analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc754b7c",
   "metadata": {},
   "source": [
    "## 4) Interactive prediction widget (ipywidgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small interactive UI to input shot features and get xG prediction from chosen model\n",
    "# Use dynamic import to avoid static-analysis missing-import diagnostics in editors\n",
    "try:\n",
    "    import importlib\n",
    "    widgets = importlib.import_module('ipywidgets')\n",
    "    ipy_disp = importlib.import_module('IPython.display')\n",
    "    display = ipy_disp.display\n",
    "    HTML = ipy_disp.HTML\n",
    "except Exception:\n",
    "    print('ipywidgets not available. To enable interactive widgets install ipywidgets.')\n",
    "    widgets = None\n",
    "\n",
    "if widgets is not None:\n",
    "    input_widgets = {}\n",
    "    for f in numeric_feats:\n",
    "        input_widgets[f] = widgets.FloatText(value=float(df[f].median()) if f in df.columns else 0.0, description=f)\n",
    "    for f in binary_feats:\n",
    "        # ensure default is one of the options to avoid TraitError\n",
    "        opts = [0, 1]\n",
    "        default = opts[0]\n",
    "        if f in df.columns:\n",
    "            try:\n",
    "                mode_val = df[f].mode().iloc[0]\n",
    "                mode_int = int(mode_val) if pd.notnull(mode_val) else default\n",
    "                if mode_int in opts:\n",
    "                    default = mode_int\n",
    "            except Exception:\n",
    "                default = opts[0]\n",
    "        input_widgets[f] = widgets.Dropdown(options=opts, value=default, description=f)\n",
    "    for f in cat_feats:\n",
    "        opts = sorted(df[f].dropna().unique().tolist()) if f in df.columns else ['missing']\n",
    "        if not opts:\n",
    "            opts = ['missing']\n",
    "        default = opts[0]\n",
    "        input_widgets[f] = widgets.Dropdown(options=opts, value=default, description=f)\n",
    "    model_select = widgets.Dropdown(options=list(models.keys()) if models else [], description='model')\n",
    "    out = widgets.Output()\n",
    "    def on_predict(_):\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            if not model_select.value:\n",
    "                print('No model selected')\n",
    "                return\n",
    "            row = {k: (w.value if not isinstance(w.value, np.ndarray) else w.value.item()) for k,w in input_widgets.items()}\n",
    "            xdf = pd.DataFrame([row])\n",
    "            mdl = models[model_select.value]\n",
    "            try:\n",
    "                p = mdl.predict_proba(xdf)[:,1][0]\n",
    "            except Exception:\n",
    "                try:\n",
    "                    p = mdl.predict_proba(preprocessor.transform(xdf))[:,1][0] if preprocessor is not None else mdl.predict_proba(xdf)[:,1][0]\n",
    "                except Exception as e:\n",
    "                    print('Prediction failed:', e)\n",
    "                    return\n",
    "            print(f'Predicted xG probability: {p:.3f}')\n",
    "    btn = widgets.Button(description='Predict')\n",
    "    btn.on_click(on_predict)\n",
    "    ui = widgets.VBox([widgets.HBox(list(input_widgets.values())), widgets.HBox([model_select, btn]), out])\n",
    "    display(ui)\n",
    "else:\n",
    "    print('Interactive widgets not available in this environment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
