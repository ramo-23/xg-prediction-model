{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a70490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and helpers\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, precision_recall_curve, roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# repo-root aware helper\n",
    "def find_repo_root(start=Path.cwd(), markers=('setup.py','requirements.txt','README.md')):\n",
    "    cur = start.resolve()\n",
    "    for _ in range(10):\n",
    "        if any((cur / m).exists() for m in markers):\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "metrics_path = repo_root / 'results' / 'metrics' / 'metrics_summary.json'\n",
    "metrics_summary = {}\n",
    "if metrics_path.exists():\n",
    "    with open(metrics_path, 'r', encoding='utf-8') as fh:\n",
    "        metrics_summary = json.load(fh)\n",
    "    print('Loaded metrics_summary.json')\n",
    "else:\n",
    "    print('metrics_summary.json not found at', metrics_path, '- run training script first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1cd8ea-1778-447f-a0fd-47fa751a276c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data and prepare features (matching training script)\n",
    "proc_dir = repo_root / 'data' / 'processed'\n",
    "files = sorted(proc_dir.glob('processed_shots_*.csv'), key=lambda p: p.stat().st_mtime)\n",
    "if not files:\n",
    "    raise FileNotFoundError('No processed_shots_*.csv files in data/processed/')\n",
    "data_path = files[-1]\n",
    "print('Using processed CSV:', data_path)\n",
    "df = pd.read_csv(data_path)\n",
    "if 'outcome' not in df.columns:\n",
    "    raise ValueError('processed data missing outcome column')\n",
    "y = df['outcome'].astype(str).str.lower().eq('goal').astype(int)\n",
    "numeric_feats = [c for c in ['distance','minute_num'] if c in df.columns]\n",
    "binary_feats = [c for c in ['body_head','body_foot','body_other','big_chance','half'] if c in df.columns]\n",
    "cat_feats = [c for c in ['shot_type','assist_type'] if c in df.columns]\n",
    "feature_cols = numeric_feats + binary_feats + cat_feats\n",
    "X = df[feature_cols].copy()\n",
    "print('Features used:', feature_cols)\n",
    "print('Class counts:', y.value_counts().to_dict())\n",
    "\n",
    "# build preprocessor\n",
    "num_pipe = Pipeline([('imp', SimpleImputer(strategy='median')), ('sc', StandardScaler())]) if numeric_feats else None\n",
    "bin_pipe = Pipeline([('imp', SimpleImputer(strategy='most_frequent'))]) if binary_feats else None\n",
    "cat_pipe = Pipeline([('imp', SimpleImputer(strategy='constant', fill_value='missing')), ('ohe', OneHotEncoder(handle_unknown='ignore'))]) if cat_feats else None\n",
    "transformers = []\n",
    "if numeric_feats: transformers.append(('num', num_pipe, numeric_feats))\n",
    "if binary_feats: transformers.append(('bin', bin_pipe, binary_feats))\n",
    "if cat_feats: transformers.append(('cat', cat_pipe, cat_feats))\n",
    "preprocessor = ColumnTransformer(transformers, remainder='drop')\n",
    "\n",
    "# splits\n",
    "X_train_full, X_hold, y_train_full, y_hold = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_hold, y_hold, test_size=0.5, stratify=y_hold, random_state=42)\n",
    "print('Train/Val/Test sizes:', len(X_train_full), len(X_val), len(X_test))\n",
    "\n",
    "# model files\n",
    "model_files = {\n",
    "    'logistic': repo_root / 'results' / 'metrics' / 'model_logistic_calibrated.joblib',\n",
    "    'random_forest': repo_root / 'results' / 'metrics' / 'model_random_forest_calibrated.joblib',\n",
    "    'xgboost': repo_root / 'results' / 'metrics' / 'model_xgboost_calibrated.joblib',\n",
    "    'neural_network': repo_root / 'results' / 'metrics' / 'model_neural_network_calibrated.joblib',\n",
    "}\n",
    "\n",
    "eval_rows = []\n",
    "for name, path in model_files.items():\n",
    "    if path.exists():\n",
    "        mdl = joblib.load(path)\n",
    "        try:\n",
    "            proba = mdl.predict_proba(X_test)[:,1]\n",
    "        except Exception:\n",
    "            try:\n",
    "                # ensure preprocessor is applied if needed\n",
    "                Xt = preprocessor.fit_transform(X_train_full) if hasattr(preprocessor, 'fit_transform') else preprocessor.transform(X_test)\n",
    "                proba = mdl.predict_proba(X_test)[:,1]\n",
    "            except Exception:\n",
    "                print('predict_proba failed for', name)\n",
    "                continue\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "        auc_score = roc_auc_score(y_test, proba) if len(np.unique(y_test))>1 else None\n",
    "        brier = brier_score_loss(y_test, proba)\n",
    "        report = classification_report(y_test, pred, output_dict=True, zero_division=0)\n",
    "        cm = confusion_matrix(y_test, pred)\n",
    "        eval_rows.append({'model': name, 'roc_auc': auc_score, 'brier': brier, 'proba': proba, 'report': report, 'cm': cm})\n",
    "    else:\n",
    "        print('Missing model file:', path)\n",
    "\n",
    "# summary\n",
    "if eval_rows:\n",
    "    df_summary = pd.DataFrame([{ 'model': r['model'], 'roc_auc': r['roc_auc'], 'brier': r['brier']} for r in eval_rows])\n",
    "    display(df_summary)\n",
    "else:\n",
    "    print('No models evaluated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5eee1",
   "metadata": {},
   "source": [
    "## ROC curves for available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "for r in eval_rows:\n",
    "    proba = r['proba']\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    plt.plot(fpr, tpr, label=f\"{r['model']} (AUC={roc_auc_score(y_test, proba):.3f})\")\n",
    "plt.plot([0,1],[0,1],'k--', alpha=0.3)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curves (test set)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95609c12",
   "metadata": {},
   "source": [
    "## Simple upsampling experiment (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90081489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling and evaluate\n",
    "train_df = pd.concat([X_train_full, y_train_full.rename('y')], axis=1)\n",
    "minor = train_df[train_df['y']==1]\n",
    "major = train_df[train_df['y']==0]\n",
    "print('Before upsample counts:', train_df['y'].value_counts().to_dict())\n",
    "if len(minor) == 0:\n",
    "    print('No positive samples in training set; skipping upsampling')\n",
    "else:\n",
    "    minor_up = resample(minor, replace=True, n_samples=len(major), random_state=42)\n",
    "    train_up = pd.concat([major, minor_up])\n",
    "    print('After upsample counts:', train_up['y'].value_counts().to_dict())\n",
    "    X_train_up = train_up[feature_cols]\n",
    "    y_train_up = train_up['y']\n",
    "    pipe_up = Pipeline([('pre', preprocessor), ('clf', LogisticRegression(max_iter=1000))])\n",
    "    pipe_up.fit(X_train_up, y_train_up)\n",
    "    proba_up = pipe_up.predict_proba(X_test)[:,1]\n",
    "    pred_up = (proba_up >= 0.5).astype(int)\n",
    "    print('Upsampled logistic ROC AUC:', roc_auc_score(y_test, proba_up))\n",
    "    print(classification_report(y_test, pred_up, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d9733",
   "metadata": {},
   "source": [
    "## Calibration and Precision-Recall Plots\n",
    "The cells below compute reliability (calibration) diagrams, predicted-probability histograms, and precisionâ€“recall curves for the evaluated models and save the figures to `results/metrics/figures/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91957852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration (reliability) diagram + probability histograms\n",
    "from sklearn.calibration import calibration_curve\n",
    "fig_dir = repo_root / 'results' / 'metrics' / 'figures'\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Reliability diagram\n",
    "plt.figure(figsize=(8,6))\n",
    "for r in eval_rows:\n",
    "    proba = r['proba']\n",
    "    prob_true, prob_pred = calibration_curve(y_test, proba, n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=f\"{r['model']} (Brier={r['brier']:.3f})\")\n",
    "plt.plot([0,1],[0,1], 'k--', alpha=0.5)\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.title('Calibration plot (reliability diagram)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "cal_fig = fig_dir / 'calibration_reliability.png'\n",
    "plt.savefig(cal_fig, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "# Predicted probability histograms\n",
    "plt.figure(figsize=(8,3))\n",
    "for r in eval_rows:\n",
    "    plt.hist(r['proba'], bins=20, alpha=0.4, label=r['model'])\n",
    "plt.legend()\n",
    "plt.title('Predicted probability distribution (test set)')\n",
    "hist_fig = fig_dir / 'probability_histograms.png'\n",
    "plt.savefig(hist_fig, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('Saved calibration and probability histogram figures to', fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curves\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "plt.figure(figsize=(8,6))\n",
    "for r in eval_rows:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, r['proba'])\n",
    "    ap = average_precision_score(y_test, r['proba'])\n",
    "    plt.plot(recall, precision, label=f\"{r['model']} (AP={ap:.3f})\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curves (test set)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "pr_fig = fig_dir / 'precision_recall_curves.png'\n",
    "plt.savefig(pr_fig, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('Saved precision-recall figure to', pr_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ad876",
   "metadata": {},
   "source": [
    "## Interpretation of Results\n",
    "This section summarizes the key numeric outcomes and provides concise interpretation and next-step recommendations based on the evaluation above. Run the cell below to see a short report and save it to `results/metrics/interpretation.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a1c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple interpretation report and save to file\n",
    "out_dir = repo_root / 'results' / 'metrics'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "report_lines = []\n",
    "# build dataframe from metrics_summary (loaded earlier)\n",
    "try:\n",
    "    df_metrics = pd.DataFrame.from_dict(metrics_summary, orient='index').reset_index().rename(columns={'index':'model'})\n",
    "except Exception:\n",
    "    df_metrics = None\n",
    "if df_metrics is not None:\n",
    "    display(df_metrics)\n",
    "    report_lines.append('Model summary (roc_auc, brier):')\n",
    "    for model, row in metrics_summary.items():\n",
    "        roc = row.get('roc_auc')\n",
    "        brier = row.get('brier')\n",
    "        report_lines.append(f'- {model}: ROC AUC={roc:.3f}' if roc is not None else f'- {model}: ROC AUC=None')\n",
    "        report_lines.append(f'  Brier={brier:.4f}')\n",
    "        cm = row.get('cm')\n",
    "        if cm is not None:\n",
    "            report_lines.append(f'  Confusion matrix: {cm}')\n",
    "    # quick interpretation heuristics\n",
    "    report_lines.append('Quick interpretation:')\n",
    "    for model, row in metrics_summary.items():\n",
    "        roc = row.get('roc_auc') or 0.0\n",
    "        brier = row.get('brier') or 1.0\n",
    "        if roc < 0.6:\n",
    "            report_lines.append(f'- {model}: Low discrimination (ROC AUC={roc:.3f}). Consider richer features or resampling/hyperparameter tuning.')\n",
    "        elif roc < 0.7:\n",
    "            report_lines.append(f'- {model}: Moderate discrimination (ROC AUC={roc:.3f}). Consider calibration and threshold tuning for downstream decisions.')\n",
    "        else:\n",
    "            report_lines.append(f'- {model}: Good discrimination (ROC AUC={roc:.3f}). Evaluate calibration and practical thresholds.')\n",
    "        if brier > 0.10:\n",
    "            report_lines.append(f'  Brier score {brier:.3f} indicates probability estimates are noisy; recalibration or more data may help.')\n",
    "else:\n",
    "    report_lines.append('No metrics available to summarize. Run the evaluation cells first.')\n",
    "# reference generated figures\n",
    "fig_dir = out_dir / 'figures'\n",
    "if fig_dir.exists():\n",
    "    report_lines.append('\\nSaved figures:')\n",
    "    for p in sorted(fig_dir.glob('*.png')):\n",
    "        report_lines.append(f'- {p.name}')\n",
    "# write to file\n",
    "int_path = out_dir / 'interpretation.txt'\n",
    "with open(int_path, 'w', encoding='utf-8') as fh:\n",
    "    fh.write('\\n'.join(report_lines))\n",
    "print('Wrote interpretation to', int_path)\n",
    "for l in report_lines:\n",
    "    print(l)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
